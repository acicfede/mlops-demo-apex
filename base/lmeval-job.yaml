apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-mmlu-granite
spec:
  # -------------------------------------------------
  # Job behavior
  # -------------------------------------------------
  allowOnline: true
  allowCodeExecution: false
  logSamples: true
  batchSize: "1"
 
  # -------------------------------------------------
  # Model type: COMPLETIONS (non chat)
  # -------------------------------------------------
  model: local-completions
 
  modelArgs:
    # ðŸ”´ DEVE combaciare con --served-model-name di vLLM
    - name: model
      value: granite-31-2b-instruct
 
    # ðŸ”´ Endpoint COMPLETIONS interno (no Route HTTPS)
    - name: base_url
      value: http://granite-31-2b-instruct-predictor.ml.svc.cluster.local:8080/v1/completions
 
    # Concurrency / retry
    - name: num_concurrent
      value: "1"
    - name: max_retries
      value: "3"
 
    # lm-eval settings
    - name: tokenized_requests
      value: "false"
 
    # Tokenizer HF (solo per conteggio token)
    - name: tokenizer
      value: ibm-granite/granite-3.1-2b-instruct
 
  # -------------------------------------------------
  # Benchmark
  # -------------------------------------------------
  taskList:
    taskNames:
      - mmlu
 
  # -------------------------------------------------
  # Output
  # -------------------------------------------------
  outputs:
    pvcManaged:
      size: 200Mi
